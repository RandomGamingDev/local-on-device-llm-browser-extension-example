# Local, On Device LLM Browser Extension Example
### An example with boilerplate for how to get LLMs in browser extensions (without the cloud) via Google's MediaPipe!

<img width="256" height="auto" alt="mediapipe-edge-ai" src="https://github.com/user-attachments/assets/c558391d-4885-4546-a15d-d76f2b07379d" />

## First Questions First: Why?
Running an edge LLM on the user's machine without the need for cloud's great for many reasons, least of which is no API cost.
|  | ☁️ Cloud/Traditional LLM ☁️ | ⚡️ Edge LLM ⚡️ | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Winner&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |
| :------- | :------: | :-------: | :--------: |
| Cost     | OpEx compound unpredictably, just **one** GPT-4 API response can cost **1.65 cents**    | **Free**<br/>including even fine-tuning due to model size    | ⚡️ Edge LLM ⚡️<br/>Nothing better than free! |
| Performance  | High raw capacity allows for **better generalized accuracy**, but at the cost of being **slow** even without the CoT often requried     | Optimized Small Language Models (SLMs) leverage **149x higher throughput**<br/> Models like [**Microsoft Phi**](https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090) and methodologies like [**"Solving a Million-Step LLM Task with Zero Errors"**](https://arxiv.org/abs/2511.09030) proving SOTA accuracy **at worst competitive with** and **at best completely exceeding ☁️ Cloud/Traditional LLM ☁️** for **domain-specific tasks (which are most), even if worse in generalized tasks**  | ⚡️ Edge LLM ⚡️ |
| Latency    | Unavoidable & unstable network cost **~50–150ms** even **ignoring** uncontrollable **initialization & congestion latency**   | **Zero network overhead** easily allowing for **sub-10ms** response times **essential** for real-time control systems and human interaction   | ⚡️ Edge LLM ⚡️ |
| Network Dependency    | **Requires high-bandwidth, continuous** internet connection, **risking buffering at best** and **complete service failure during outages at worst**   | Guarantees **100% operational independence**, ensuring continuous inference and local functionality, **even when completely offline**   | ⚡️ Edge LLM ⚡️ |
| Customization    | **Practically no flexibility** as **proprietary APIs restrict access to model weights**, making **domain-specific fine-tuning expensive or impossible**    | **Full, direct control** over the model stack (GGUF, quantization), allowing **deep customization** for proprietary datasets and core business logic   | ⚡️ Edge LLM ⚡️ |
| Community    | **Vendor-reliant** requiring **provider-specific** documentation and development, with often **poor track records and walked-back decisions**, **restricting or blocking** community involvement   | **Thriving open-source ecosystems** (e.g., Llama.cpp, KAITO) provide **rapid innovation, broad toolchains, and peer-driven solutions larger & more accessible than proprietary solutions**  | ⚡️ Edge LLM ⚡️ |
| Privacy    | **Requires** trusting **proven-untrustworthy companies and their 3rd parties** as well as transmission over networks, **potentially violating data sovereignty, compliance, and residency mandates**    | **All data's local** and **complete regulatory control** (GDPR, HIPAA, etc.)   | ⚡️ Edge LLM ⚡️ |
| Censorship    | Supplier-imposed **guardrails & content-filtering**, **blocking even legitimate uses**    | **No or configurable guardrails**, allowing **fine-grained control**   | ⚡️ Edge LLM ⚡️ |
| Supplier flexibility    | **Vendor lock-in** due to API specificity and **proprietary model dependency**, resulting in **high switching costs**   | **Open standards & portable**, enabling **seamless adoption of superior models**  | ⚡️ Edge LLM ⚡️ |
| Redundancy    | **Centralized point of failure** with extensive multi-region deployment strategies to mitigate single-vendor outages, and still **countless failures**   | On device means **never fails** even if the whole internet dies  | ⚡️ Edge LLM ⚡️ |
| Environment    | **Extreme cumulative [energy](https://www.hecweb.org/2025/06/28/measuring-the-environmental-cost-of-artificial-intelligence-and-their-data-centers/#:~:text=On%20the%20front%20lines%20of,rezoned%20to%20make%20way%20for) & [water consumption](https://www.lincolninst.edu/publications/land-lines-magazine/articles/land-water-impacts-data-centers/)** of massive data centers **[destroying environments](https://www.hecweb.org/2025/06/28/measuring-the-environmental-cost-of-artificial-intelligence-and-their-data-centers/#:~:text=On%20the%20front%20lines%20of,rezoned%20to%20make%20way%20for), [homes](https://ace-ej.org/ai-dark-side-climate-chaos/#:~:text=One%20new%20data%20center%2C%20in,heart%20disease%2C%20and%20early%20death.), [communites](https://www.smithsonianmag.com/science-nature/with-ai-on-the-rise-what-will-be-the-environmental-impacts-of-data-centers-180987379/), and our [planet](https://www.unep.org/news-and-stories/story/ai-has-environmental-problem-heres-what-world-can-do-about)** and still projected to reach [**petawatt-hour levels by 2026 globally**](https://www.iea.org/reports/energy-and-ai)   | Localized processing means **energy costs magnitudes lower than the ☁️ Cloud/Traditional LLM ☁️** costs while **enabling optimization** (e.g. Energy Delay Product (EDP))  | ⚡️ Edge LLM ⚡️ |

These cover nearly all of **what creators & consumers want** in  AI while simply being the **de facto moral choice** whether it comes to monopolies, economy, privacy, environment, or creative expression.

## How to Run the Demo:
1. `npm install`
2. `npm run build`
3. Load the extension as a temporary one from `about:debugging#/runtime/this-firefox` for Firefox and `chrome://extensions` for Chrome (make sure to turn on `Developer Mode` in the top right)
4. Click on the extension's icon in order to open a chat window where you can type into the top text field and then press the "Get Response" button to get the LLM's generated response
<img width="681" height="641" alt="image" src="https://github.com/user-attachments/assets/0a4266b7-e052-41de-a0f7-a857734ba7e6" />
